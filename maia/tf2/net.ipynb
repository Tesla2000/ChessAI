{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1c464e-5c7d-427e-b3e6-a36266255be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def make_const(shape, values, dtype=tf.float32, name=None):\n",
    "    return tf.constant(values, shape=shape, dtype=dtype, name=name)\n",
    "\n",
    "def make_int_const(shape, values, dtype=tf.int32, name=None):\n",
    "    return tf.constant(values, shape=shape, dtype=dtype, name=name)\n",
    "\n",
    "def squeeze_and_excite(input_tensor, channels, se_weights, name, cpu=False):\n",
    "    with tf.name_scope(name):\n",
    "        pooled = tf.reduce_mean(input_tensor, axis=[2, 3] if cpu else [1, 2], keepdims=True)\n",
    "        w1 = make_const([channels, se_weights['se_channels']], se_weights['w1'])\n",
    "        b1 = make_const([se_weights['se_channels']], se_weights['b1'])\n",
    "        dense1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(pooled, w1), b1))\n",
    "        \n",
    "        w2 = make_const([se_weights['se_channels'], 2 * channels], se_weights['w2'])\n",
    "        b2 = make_const([2 * channels], se_weights['b2'])\n",
    "        dense2 = tf.nn.bias_add(tf.matmul(dense1, w2), b2)\n",
    "        scale = tf.sigmoid(dense2[:, :, :, :channels])\n",
    "        excitation = scale * input_tensor\n",
    "        \n",
    "        return excitation\n",
    "\n",
    "def make_conv_block(input_tensor, channels, input_channels, output_channels, weights, name, relu=True, seunit=None):\n",
    "    with tf.name_scope(name):\n",
    "        w_conv = make_const([channels, channels, input_channels, output_channels], weights['conv_weights'])\n",
    "        conv2d = tf.nn.conv2d(input_tensor, w_conv, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        b_conv = make_const([output_channels], weights['conv_biases'])\n",
    "        conv_with_bias = tf.nn.bias_add(conv2d, b_conv)\n",
    "        \n",
    "        if seunit:\n",
    "            conv_with_bias = squeeze_and_excite(conv_with_bias, output_channels, seunit, name + \"/se\")\n",
    "        \n",
    "        if relu:\n",
    "            return tf.nn.relu(conv_with_bias)\n",
    "        else:\n",
    "            return conv_with_bias\n",
    "\n",
    "def make_residual_block(input_tensor, channels, weights, name, cpu=False):\n",
    "    with tf.name_scope(name):\n",
    "        conv1 = make_conv_block(input_tensor, 3, channels, channels, weights['conv1'], name + \"/conv1\", relu=True)\n",
    "        conv2 = make_conv_block(conv1, 3, channels, channels, weights['conv2'], name + \"/conv2\", relu=False, seunit=weights.get('se', None))\n",
    "        return tf.nn.relu(conv2 + input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0d5831-eaa3-41db-b3d6-e6441e9b2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1, 64, 64, 3)\n",
      "Output tensor: tf.Tensor(\n",
      "[[[[ 0.          1.7313688   0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 1.7498739   0.          0.        ]\n",
      "   [ 0.          0.5159397   0.        ]]\n",
      "\n",
      "  [[ 0.          7.5680194   0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ...\n",
      "   [ 0.         11.978024   21.85036   ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[14.911353    7.839838    0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.         23.082615    0.6489618 ]\n",
      "   ...\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 3.7793534   0.          0.        ]\n",
      "   [ 0.72525287  0.         16.881302  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[10.655182    0.          0.        ]\n",
      "   [ 0.         17.910152   26.609123  ]\n",
      "   [ 0.         16.996769    0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.         10.201911    0.        ]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          9.320237    0.        ]\n",
      "   [ 0.          2.621275   46.76396   ]\n",
      "   [15.002923    0.          0.        ]\n",
      "   ...\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.6710967   0.          0.        ]]\n",
      "\n",
      "  [[14.67703     0.          0.        ]\n",
      "   [ 0.         12.021568    0.        ]\n",
      "   [ 0.          3.7387571   0.        ]\n",
      "   ...\n",
      "   [ 0.         22.185331    0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 1.1781945   2.5454726   0.        ]]]], shape=(1, 64, 64, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def test_residual_block():\n",
    "    # Create a dummy input tensor with random data\n",
    "    # Shape [batch_size, height, width, channels]\n",
    "    input_tensor = tf.random.normal([1, 64, 64, 3])\n",
    "\n",
    "    # Define dummy weights for the convolutional layers\n",
    "    weights = {\n",
    "        'conv1': {\n",
    "            'conv_weights': tf.random.normal([3, 3, 3, 3]),\n",
    "            'conv_biases': tf.random.normal([3]),\n",
    "        },\n",
    "        'conv2': {\n",
    "            'conv_weights': tf.random.normal([3, 3, 3, 3]),\n",
    "            'conv_biases': tf.random.normal([3]),\n",
    "            'se': {\n",
    "                'se_channels': 4,\n",
    "                'w1': tf.random.normal([3, 4]),\n",
    "                'b1': tf.random.normal([4]),\n",
    "                'w2': tf.random.normal([4, 6]),\n",
    "                'b2': tf.random.normal([6]),\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Run the residual block function\n",
    "    output = make_residual_block(input_tensor, 3, weights, \"test_block\")\n",
    "\n",
    "    # Print the output shape and some details to verify correctness\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output tensor:\", output)\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_residual_block()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47865e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\rafal\\\\Python projects\\\\chess_xd\\\\chess\\\\lczero_training2\\\\lczero-training\\\\tf2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e981d902-256b-4201-a6dc-83da803b11e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  input_test: PATH_TO_VAL_FILES\n",
      "  input_train: PATH_TO_TRAINING_FILES\n",
      "gpu: 0\n",
      "model:\n",
      "  filters: 64\n",
      "  residual_blocks: 6\n",
      "  se_ratio: 8\n",
      "name: kb1-64x6\n",
      "training:\n",
      "  batch_size: 1024\n",
      "  checkpoint_steps: 10000\n",
      "  lr_boundaries:\n",
      "  - 80000\n",
      "  - 200000\n",
      "  - 360000\n",
      "  lr_values:\n",
      "  - 0.1\n",
      "  - 0.01\n",
      "  - 0.001\n",
      "  - 0.0001\n",
      "  num_batch_splits: 1\n",
      "  path: C:\\Users\\rafal\\Python projects\\chess_xd\\chess\\lczero_training2\\lczero-training\\networks\n",
      "  policy_loss_weight: 1.0\n",
      "  precision: half\n",
      "  shuffle_size: 250000\n",
      "  test_steps: 2000\n",
      "  total_steps: 400000\n",
      "  train_avg_report_steps: 50\n",
      "  value_loss_weight: 1.0\n",
      "\n",
      "Wrote model to C:\\Users\\rafal\\Python projects\\chess_xd\\chess\\lczero_training2\\lczero-training\\networks\\kb1-64x6\\kb1-64x6-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 20:30:57.770283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\n",
      "2024-06-08 20:30:59.849237: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll\n",
      "2024-06-08 20:31:00.326777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3050 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.5GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2024-06-08 20:31:00.327223: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll\n",
      "2024-06-08 20:31:00.332602: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll\n",
      "2024-06-08 20:31:00.332707: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll\n",
      "2024-06-08 20:31:00.335308: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll\n",
      "2024-06-08 20:31:00.336081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll\n",
      "2024-06-08 20:31:00.340121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll\n",
      "2024-06-08 20:31:00.341606: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll\n",
      "2024-06-08 20:31:00.342331: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll\n",
      "2024-06-08 20:31:00.342685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-06-08 20:31:00.344255: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-08 20:31:00.345113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3050 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.5GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2024-06-08 20:31:00.345544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2024-06-08 20:31:01.379773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-06-08 20:31:01.379969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2024-06-08 20:31:01.380088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2024-06-08 20:31:01.380355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1646 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    }
   ],
   "source": [
    "!python net_to_model.py --cfg \"configs/maia_config.yaml\" \"maia-1800.pb.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353586cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafal\\Python projects\\chess_xd\\chess\\.venv\\Scripts;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.5\\libnvvp;C:\\Program Files\\NVIDIA\\CUDNN\\v9.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.3\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Git\\cmd;C:\\Users\\rafal\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\rafal\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Users\\rafal\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;C:\\Program Files\\protoc\\bin;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2020.3.0\\;\n"
     ]
    }
   ],
   "source": [
    "%echo %PATH%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498cc926-a14f-4afd-865a-5a94d3b60bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .pb.gz network\n",
      "Blocks: 6\n",
      "Filters: 64\n",
      "pblczero.Net.magic: 448\n",
      "pblczero.Net.license: \n",
      "pblczero.EngineVersion.major: 0\n",
      "pblczero.EngineVersion.minor: 21\n",
      "pblczero.EngineVersion.patch: 0\n",
      "pblczero.Format.weights_encoding: LINEAR16\n",
      "pblczero.NetworkFormat.input: INPUT_CLASSICAL_112_PLANE\n",
      "pblczero.NetworkFormat.output: OUTPUT_WDL\n",
      "pblczero.NetworkFormat.network: NETWORK_SE_WITH_HEADFORMAT\n",
      "pblczero.NetworkFormat.policy: POLICY_CONVOLUTION\n",
      "pblczero.NetworkFormat.value: VALUE_WDL\n",
      "pblczero.NetworkFormat.moves_left: MOVES_LEFT_NONE\n",
      "pblczero.TrainingParams.training_steps: 400000\n",
      "pblczero.TrainingParams.learning_rate: 9.999999747378752e-05\n",
      "pblczero.TrainingParams.mse_loss: 0.20176962018013\n",
      "pblczero.TrainingParams.policy_loss: 1.4583964347839355\n",
      "pblczero.TrainingParams.accuracy: 52.52495574951172\n",
      "pblczero.TrainingParams.lc0_params: \n",
      "\n",
      "Writing output to: maia-1800.txt.gz\n",
      "saved as 'maia-1800.txt.gz' 4.83M\n"
     ]
    }
   ],
   "source": [
    "!python net.py -i \"maia-1800.pb.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f73746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "for gpu in gpus:\n",
    "    print(gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa700be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08fa14c-e7af-4395-8971-e0bf3956c8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting GPU: Physical devices cannot be modified after being initialized, falling back to CPU.\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3050 Laptop GPU, compute capability 8.6\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rafal\\anaconda3\\envs\\chessenv\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tfprocess import TFProcess \n",
    "import yaml\n",
    "\n",
    "# Create a new model instance (ensure it has the same architecture as the saved model)\n",
    "config_path = \"configs/maia_config.yaml\"\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "model = TFProcess(config)\n",
    "\n",
    "# Load the weights from the checkpoint\n",
    "checkpoint_path = \"maia-1800.pb.gz\"\n",
    "model.init_net_v2()\n",
    "\n",
    "model.replace_weights_v2(checkpoint_path)\n",
    "\n",
    "model.model.save('mymodel2.h5', save_format='h5')\n",
    "# Now the model's weights have been restored and it is ready to be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbbf3088-76d8-49c1-9d83-0fadf8921044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from tfprocess import ApplySqueezeExcitation, ApplyPolicyMap\n",
    "\n",
    "# Define a dictionary mapping the name of the custom layers to the actual layers\n",
    "custom_objects_dict = {\n",
    "    'ApplySqueezeExcitation': ApplySqueezeExcitation,\n",
    "    'ApplyPolicyMap': ApplyPolicyMap\n",
    "}\n",
    "\n",
    "# Use the custom_objects argument to load the model\n",
    "loaded_model = tf.keras.models.load_model('mymodel2.h5', custom_objects=custom_objects_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c8355c-983f-43bb-95e3-8516a44876c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 112, 64)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 112, 8, 8)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input/conv2d (Conv2D)           (None, 64, 8, 8)     64512       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input/bn (BatchNormalization)   (None, 64, 8, 8)     256         input/conv2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 8, 8)     0           input/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_1/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 8, 8)     0           residual_1/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_1/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           residual_1/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8)            0           residual_1/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_1/se/se/dense2 (Dense) (None, 128)          1152        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation (Apply (None, 64, 8, 8)     0           residual_1/2/bn[0][0]            \n",
      "                                                                 residual_1/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 8, 8)     0           activation[0][0]                 \n",
      "                                                                 apply_squeeze_excitation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 8, 8)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_2/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 8, 8)     0           residual_2/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_2/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 64)           0           residual_2/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8)            0           residual_2/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_2/se/se/dense2 (Dense) (None, 128)          1152        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation_1 (App (None, 64, 8, 8)     0           residual_2/2/bn[0][0]            \n",
      "                                                                 residual_2/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 8, 8)     0           activation_3[0][0]               \n",
      "                                                                 apply_squeeze_excitation_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 8, 8)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_3/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 8, 8)     0           residual_3/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_3/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 64)           0           residual_3/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8)            0           residual_3/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_3/se/se/dense2 (Dense) (None, 128)          1152        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation_2 (App (None, 64, 8, 8)     0           residual_3/2/bn[0][0]            \n",
      "                                                                 residual_3/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 8, 8)     0           activation_6[0][0]               \n",
      "                                                                 apply_squeeze_excitation_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 8, 8)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_4/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 64, 8, 8)     0           residual_4/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_4/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 64)           0           residual_4/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8)            0           residual_4/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_4/se/se/dense2 (Dense) (None, 128)          1152        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation_3 (App (None, 64, 8, 8)     0           residual_4/2/bn[0][0]            \n",
      "                                                                 residual_4/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 8, 8)     0           activation_9[0][0]               \n",
      "                                                                 apply_squeeze_excitation_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 8, 8)     0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_5/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 8, 8)     0           residual_5/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_5/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 64)           0           residual_5/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8)            0           residual_5/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_5/se/se/dense2 (Dense) (None, 128)          1152        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation_4 (App (None, 64, 8, 8)     0           residual_5/2/bn[0][0]            \n",
      "                                                                 residual_5/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 8, 8)     0           activation_12[0][0]              \n",
      "                                                                 apply_squeeze_excitation_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 8, 8)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/1/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/1/bn (BatchNormaliza (None, 64, 8, 8)     192         residual_6/1/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 8, 8)     0           residual_6/1/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/2/conv2d (Conv2D)    (None, 64, 8, 8)     36864       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/2/bn (BatchNormaliza (None, 64, 8, 8)     256         residual_6/2/conv2d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 64)           0           residual_6/2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/se/se/dense1 (Dense) (None, 8)            520         global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8)            0           residual_6/se/se/dense1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "residual_6/se/se/dense2 (Dense) (None, 128)          1152        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "apply_squeeze_excitation_5 (App (None, 64, 8, 8)     0           residual_6/2/bn[0][0]            \n",
      "                                                                 residual_6/se/se/dense2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 8, 8)     0           activation_15[0][0]              \n",
      "                                                                 apply_squeeze_excitation_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 8, 8)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "value/conv2d (Conv2D)           (None, 32, 8, 8)     2048        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "policy1/conv2d (Conv2D)         (None, 64, 8, 8)     36864       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "value/bn (BatchNormalization)   (None, 32, 8, 8)     96          value/conv2d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "policy1/bn (BatchNormalization) (None, 64, 8, 8)     192         policy1/conv2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 8, 8)     0           value/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 8, 8)     0           policy1/bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "policy (Conv2D)                 (None, 80, 8, 8)     46160       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "value/dense1 (Dense)            (None, 128)          262272      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "apply_policy_map (ApplyPolicyMa (None, 1858)         0           policy[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "value/dense2 (Dense)            (None, 3)            387         value/dense1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 867,875\n",
      "Trainable params: 866,019\n",
      "Non-trainable params: 1,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import lc0_az_policy_map\n",
    "\n",
    "# Assuming ApplySqueezeExcitation is defined somewhere in your script\n",
    "class ApplySqueezeExcitation(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ApplySqueezeExcitation, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.reshape_size = input_shape[1][1]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        excited = inputs[1]\n",
    "        gammas, betas = tf.split(tf.reshape(excited, [-1, self.reshape_size, 1, 1]), 2, axis=1)\n",
    "        return tf.nn.sigmoid(gammas) * x + betas\n",
    "\n",
    "class ApplyPolicyMap(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ApplyPolicyMap, self).__init__(**kwargs)\n",
    "        self.fc1 = tf.constant(lc0_az_policy_map.make_map())  # Ensure lc0_az_policy_map.make_map() is defined or available\n",
    "\n",
    "    def call(self, inputs):\n",
    "        h_conv_pol_flat = tf.reshape(inputs, [-1, 80 * 8 * 8])\n",
    "        return tf.matmul(h_conv_pol_flat, tf.cast(self.fc1, h_conv_pol_flat.dtype))\n",
    "\n",
    "# Loading the model with both custom layers\n",
    "loaded_model = tf.keras.models.load_model('mymodel2.h5', custom_objects={\n",
    "    'ApplySqueezeExcitation': ApplySqueezeExcitation,\n",
    "    'ApplyPolicyMap': ApplyPolicyMap\n",
    "})\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a13990-659d-47d9-bb45-92e4c2d80154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input layer name: input_1\n",
      "Input shape: [(None, 112, 64)]\n",
      "Input dtype: float32\n",
      "Model input shape: (None, 112, 64)\n",
      "Model input dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "for layer in loaded_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "        print(f\"Input layer name: {layer.name}\")\n",
    "        print(f\"Input shape: {layer.input_shape}\")\n",
    "        print(f\"Input dtype: {layer.dtype}\")\n",
    "\n",
    "# Alternatively, you can directly access the model's input details\n",
    "print(f\"Model input shape: {loaded_model.input_shape}\")\n",
    "print(f\"Model input dtype: {loaded_model.inputs[0].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79e506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "64\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "input = '''0000000000000000000000000001000000000000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000001000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000000000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000100000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001000000000000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000001000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000000000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000100000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001000000000000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000000000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000100000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001100000000000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000000000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000100000000000000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001000000010000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000000000000100100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000100000000000000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001100000010000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000001000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000100000000000000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001000000011000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000001000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000100000000000000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000001000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000001000000011000010000011000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000100000100000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000100000 1\n",
    "0000000000000000000000000000000000000000000001000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000001000000 1\n",
    "0000000011100000000001000000000100000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000100000000000000000000000000000000000000000000 1\n",
    "0001000100000000000000000000000000000000000000000000000000000000 1\n",
    "0000100000000000000000000000000000000000000000000000000000000000 1\n",
    "0100000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "0000000000000000000000000000000000000000000000000000000000000000 0\n",
    "0000000000000000000000000000000000000000000000000000000000000000 1\n",
    "1111111111111111111111111111111111111111111111111111111111111111 1'''.split('\\n')\n",
    "\n",
    "data = [[float(value) for value in element.split(\" \")[0]] for element in input]\n",
    "\n",
    "\n",
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1181f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb8b054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 64)\n",
      "(1, 112, 64)\n",
      "[array([[ -5.746  ,  -3.064  ,  -2.979  ,  -1.75   ,  -1.854  ,  -1.568  ,\n",
      "         -0.952  ,  -2.885  ,  -0.918  ,  -0.3582 ,  -4.28   ,  -0.677  ,\n",
      "         -3.498  ,  -3.434  ,  -2.697  ,  -2.73   ,  -2.29   ,  -2.617  ,\n",
      "         -1.386  ,  -2.041  ,  -1.076  ,  -0.695  ,  -0.08655,  -4.633  ,\n",
      "         -4.67   ,  -2.932  ,  -1.732  ,  -1.308  ,  -1.691  ,  -1.365  ,\n",
      "         -1.373  ,  -3.82   ,  -3.428  ,  -1.825  ,  -0.3796 ,  -5.16   ,\n",
      "         -0.5215 ,  -4.38   ,  -3.92   ,  -3.014  ,  -3.223  ,  -2.947  ,\n",
      "         -2.58   ,  -1.192  ,  -1.98   ,  -1.207  ,  -0.4258 ,  -3.547  ,\n",
      "         -5.656  ,  -3.715  ,  -2.822  ,  -1.18   ,  -1.177  ,  -1.445  ,\n",
      "         -1.1045 ,  -1.482  ,  -3.16   ,  -6.965  ,   1.612  ,  -4.133  ,\n",
      "         -2.2    ,  -3.756  ,  -2.918  ,  -2.293  ,  -3.63   ,  -1.317  ,\n",
      "         -3.037  ,  -0.884  ,  -2.72   ,  -0.27   ,  -2.287  ,  -1.898  ,\n",
      "         -2.047  ,  -3.133  ,  -4.54   ,  -3.398  ,  -2.385  ,  -0.9375 ,\n",
      "         -1.03   ,  -0.04272,  -3.309  ,  -2.637  ,  -0.658  ,  -1.198  ,\n",
      "         -3.148  ,   0.1523 ,  -1.632  ,  -0.1931 ,  -3.984  ,  -3.129  ,\n",
      "         -2.947  ,  -2.396  ,  -2.594  ,  -2.156  ,  -2.426  ,  -1.643  ,\n",
      "         -0.786  ,  -2.363  ,  -2.08   ,  -4.004  ,  -4.535  ,  -4.37   ,\n",
      "         -2.762  ,  -1.645  ,  -1.017  ,  -6.062  ,  -2.287  ,  -6.57   ,\n",
      "         -3.645  ,  -3.2    ,  -2.854  ,  -2.72   ,  -3.1    ,  -3.242  ,\n",
      "         -2.367  ,  -3.748  ,  -2.29   ,  -2.094  ,  -2.5    ,  -2.69   ,\n",
      "         -1.82   ,  -2.21   ,  -1.997  ,  -1.722  ,  -2.473  ,  -2.555  ,\n",
      "         -4.594  ,  -3.54   ,  -2.793  ,  -2.924  ,   0.3228 ,  -1.992  ,\n",
      "         -6.65   ,  -4.32   ,  -4.184  ,  -3.102  ,  -1.675  ,  -3.197  ,\n",
      "         -3.24   ,  -3.07   ,  -2.8    ,  -1.774  ,  -1.796  ,  -2.467  ,\n",
      "         -2.305  ,  -1.818  ,  -2.383  ,  -2.127  ,  -2.455  ,  -1.723  ,\n",
      "         -2.148  ,  -3.023  ,  -5.125  ,  -3.816  ,   0.914  ,  -7.555  ,\n",
      "         -1.68   ,  -6.676  ,  -3.535  ,  -3.828  ,  -2.334  ,  -3.53   ,\n",
      "         -2.771  ,  -3.209  ,  -2.084  ,  -2.516  ,  -2.387  ,  -2.287  ,\n",
      "         -1.619  ,  -2.34   ,  -2.154  ,  -2.246  ,  -2.188  ,  -1.875  ,\n",
      "         -1.639  ,  -2.066  ,  -2.875  ,  -4.457  ,  -2.1    ,  -7.574  ,\n",
      "         -1.322  ,  -3.297  ,  -1.597  ,  -1.8    ,  -2.64   ,  -2.955  ,\n",
      "         -2.035  ,  -2.29   ,  -2.43   ,  -2.387  ,  -1.654  ,  -1.916  ,\n",
      "         -0.293  ,  -1.732  ,  -5.18   ,  -6.7    ,  -3.484  ,  -3.434  ,\n",
      "         -4.92   ,  -4.715  ,  -2.904  ,  -2.645  ,  -2.873  ,  -3.766  ,\n",
      "         -3.258  ,  -7.598  ,  -1.689  ,  -1.476  ,  -1.2295 ,  -4.105  ,\n",
      "         -3.963  ,  -3.846  ,  -2.848  ,  -2.648  ,  -2.883  ,  -2.053  ,\n",
      "         -3.273  ,  -1.495  ,  -6.52   ,  -5.562  ,  -7.57   ,  -3.264  ,\n",
      "         -2.299  ,  -3.855  ,  -4.547  ,  -2.139  ,  -2.615  ,  -2.533  ,\n",
      "         -2.67   ,  -7.332  ,  -4.715  ,  -8.61   ,  -3.31   ,  -2.258  ,\n",
      "         -2.523  ,  -1.745  ,  -5.863  ,  -3.686  ,  -4.273  ,  -2.412  ,\n",
      "         -3.395  ,  -2.607  ,  -2.215  ,  -2.982  ,  -1.791  ,  -2.934  ,\n",
      "         -6.355  ,  -5.254  ,  -6.008  ,  -4.31   ,  -5.395  ,  -3.436  ,\n",
      "         -6.258  ,  -0.7217 ,  -3.348  ,  -3.543  ,  -3.209  ,  -2.379  ,\n",
      "         -7.13   ,  -2.924  ,  -7.598  ,  -3.445  ,  -4.55   ,  -2.56   ,\n",
      "         -0.6504 ,  -5.473  ,  -4.047  ,  -4.332  ,  -3.963  ,  -3.459  ,\n",
      "         -3.629  ,  -3.254  ,  -2.271  ,  -3.162  ,  -5.     ,  -6.055  ,\n",
      "         -4.816  ,  -5.957  ,  -6.293  ,  -3.824  ,  -3.709  ,  -3.396  ,\n",
      "          1.008  ,  -3.988  ,  -3.643  ,  -3.436  ,  -5.73   ,  -7.453  ,\n",
      "         -1.561  ,  -7.25   ,  -6.61   ,  -3.104  ,  -4.973  ,  -2.408  ,\n",
      "         -4.98   ,  -4.01   ,  -3.477  ,  -3.861  ,  -3.275  ,  -3.26   ,\n",
      "         -3.164  ,  -3.262  ,  -3.354  ,  -3.785  ,  -5.17   ,  -6.176  ,\n",
      "         -7.203  ,  -4.93   ,  -3.355  ,  -3.262  ,  -3.514  ,  -6.35   ,\n",
      "         -6.574  ,  -4.434  ,  -3.742  ,  -2.916  ,  -8.21   ,  -4.4    ,\n",
      "         -8.01   ,  -5.36   ,  -4.13   ,  -6.43   ,  -2.637  ,  -3.814  ,\n",
      "         -5.086  ,  -3.898  ,  -3.582  ,  -4.35   ,  -3.402  ,  -2.531  ,\n",
      "         -2.361  ,  -2.553  ,  -3.941  ,  -5.555  ,  -5.004  ,  -5.457  ,\n",
      "         -5.496  ,  -3.066  ,  -3.057  ,  -2.775  ,  -4.19   ,   0.4993 ,\n",
      "         -5.     ,  -3.672  ,  -5.28   ,  -7.195  ,  -2.02   ,  -7.25   ,\n",
      "         -5.773  ,  -3.691  ,  -4.33   ,  -0.9404 ,  -5.465  ,  -3.838  ,\n",
      "         -2.875  ,  -2.51   ,  -2.326  ,  -2.465  ,  -2.455  ,  -2.064  ,\n",
      "         -1.9795 ,  -4.973  ,  -5.305  ,  -4.69   ,  -5.234  ,  -3.22   ,\n",
      "         -2.58   ,  -2.703  ,  -2.568  ,  -0.0649 ,  -5.22   ,  -5.83   ,\n",
      "         -4.555  ,  -6.727  ,  -3.379  ,  -6.547  ,  -3.207  ,  -3.934  ,\n",
      "         -1.5    ,  -4.562  ,  -2.54   ,  -3.514  ,  -2.447  ,  -2.908  ,\n",
      "         -1.877  ,  -2.99   ,  -1.275  ,  -3.266  ,  -4.88   ,  -3.934  ,\n",
      "         -2.973  ,  -2.42   ,  -2.018  ,  -2.223  ,  -1.414  ,  -0.05493,\n",
      "         -3.559  ,  -4.848  ,  -4.938  ,  -6.36   ,  -1.564  ,  -2.975  ,\n",
      "         -4.57   ,  -0.676  ,  -1.471  ,  -3.416  ,  -1.645  ,  -2.75   ,\n",
      "         -2.043  ,  -2.81   ,  -1.231  ,  -2.57   ,  -4.727  ,  -5.203  ,\n",
      "         -3.807  ,  -2.26   ,  -1.666  ,  -3.342  ,  -4.188  ,  -2.107  ,\n",
      "         -2.168  ,  -2.266  ,  -2.342  ,  -2.105  ,  -2.73   ,  -1.598  ,\n",
      "         -4.97   ,  -4.855  ,  -2.871  ,  -4.58   ,  -2.473  ,  -2.957  ,\n",
      "         -2.26   ,  -2.824  ,  -1.905  ,  -2.887  ,  -1.645  ,  -5.773  ,\n",
      "         -5.58   ,  -5.977  ,  -3.918  ,  -1.015  ,  -2.678  ,  -1.231  ,\n",
      "         -5.582  ,  -3.709  ,  -4.64   ,  -3.225  ,  -2.402  ,  -2.314  ,\n",
      "         -2.516  ,  -2.55   ,  -4.414  ,  -2.31   ,  -5.74   ,  -7.883  ,\n",
      "         -5.07   ,  -2.568  ,  -4.82   ,  -3.312  ,  -3.447  ,  -2.742  ,\n",
      "         -2.404  ,  -3.078  ,  -3.227  ,  -2.326  ,  -4.273  ,  -6.61   ,\n",
      "         -4.406  ,  -6.344  ,  -2.705  ,  -4.934  ,  -2.725  ,  -3.053  ,\n",
      "         -4.52   ,  -2.293  ,  -2.977  ,  -4.848  ,  -5.117  ,  -2.576  ,\n",
      "         -2.92   ,  -2.357  ,  -2.916  ,  -7.285  ,  -4.926  ,  -2.51   ,\n",
      "         -9.195  ,  -6.945  ,  -3.639  ,  -7.258  ,  -3.287  ,  -7.582  ,\n",
      "         -6.793  ,  -3.643  ,  -5.594  ,  -1.466  ,  -5.68   ,  -2.986  ,\n",
      "         -3.494  ,  -4.594  ,  -4.31   ,  -3.17   ,  -4.812  ,  -4.676  ,\n",
      "         -2.97   ,  -3.84   ,  -4.387  ,   0.5127 ,  -5.203  ,  -3.451  ,\n",
      "         -3.55   ,  -4.797  ,  -5.06   ,  -3.71   ,  -3.273  ,  -2.578  ,\n",
      "         -4.07   ,  -6.03   ,  -7.023  ,  -6.113  ,  -4.24   ,  -2.365  ,\n",
      "         -3.178  ,  -5.59   ,  -2.854  ,  -2.555  ,  -3.537  ,  -5.36   ,\n",
      "         -3.12   ,  -5.83   ,  -2.27   ,  -5.113  ,  -2.799  ,  -3.87   ,\n",
      "         -4.375  ,  -4.99   ,  -3.26   ,  -2.932  ,  -4.098  ,   0.494  ,\n",
      "         -4.273  ,  -5.273  ,  -2.615  ,  -2.914  ,  -3.127  ,  -5.836  ,\n",
      "         -5.332  ,  -3.879  ,  -3.781  ,  -4.81   ,  -6.63   ,  -3.484  ,\n",
      "         -3.934  ,  -5.676  ,  -4.758  ,  -4.45   ,  -4.44   ,  -4.793  ,\n",
      "         -2.291  ,  -4.227  ,  -3.205  ,  -2.451  ,  -3.82   ,  -1.569  ,\n",
      "         -3.107  ,  -3.414  ,  -4.812  ,  -4.42   ,  -4.652  ,  -4.855  ,\n",
      "         -3.674  ,  -0.425  ,  -5.566  ,  -7.33   ,  -5.574  ,  -2.709  ,\n",
      "         -2.547  ,  -2.66   ,  -4.11   ,  -6.297  ,  -6.902  ,  -4.496  ,\n",
      "         -6.473  ,  -8.31   ,  -3.39   ,  -6.562  ,  -5.652  ,  -3.436  ,\n",
      "         -3.242  ,  -4.324  ,  -4.54   ,  -6.63   ,  -5.164  ,  -4.27   ,\n",
      "         -1.87   ,  -2.375  ,  -2.309  ,  -2.84   ,  -3.36   ,  -5.68   ,\n",
      "         -4.203  ,  -5.254  ,  -0.4346 ,  -3.984  ,  -4.277  ,  -4.78   ,\n",
      "         -2.865  ,  -2.174  ,  -2.023  ,  -2.688  ,  -3.428  ,  -4.754  ,\n",
      "         -4.215  ,  -5.633  ,  -5.918  ,  -2.443  ,  -2.52   ,  -1.223  ,\n",
      "         -5.504  ,  -3.69   ,  -5.465  ,  -1.658  ,  -3.127  ,  -1.666  ,\n",
      "         -2.744  ,  -2.352  ,  -2.752  ,  -3.137  ,  -5.027  ,  -3.518  ,\n",
      "         -4.91   ,  -4.992  ,  -5.086  ,  -2.51   ,  -2.572  ,  -2.346  ,\n",
      "         -2.498  ,  -3.076  ,  -3.664  ,  -5.86   ,  -4.48   ,  -4.67   ,\n",
      "         -2.781  ,  -2.998  ,  -4.504  ,  -4.152  ,  -1.587  ,  -3.555  ,\n",
      "         -1.849  ,  -3.725  ,  -2.547  ,  -3.064  ,  -2.21   ,  -1.103  ,\n",
      "         -1.604  ,  -2.11   ,  -0.3687 ,  -2.084  ,  -2.773  ,  -3.32   ,\n",
      "         -1.724  ,  -1.809  ,  -1.289  ,  -0.97   ,  -1.665  ,  -2.105  ,\n",
      "         -2.125  ,  -1.344  ,  -4.8    ,  -2.63   ,  -0.705  ,  -6.688  ,\n",
      "         -1.756  ,  -2.166  ,  -1.79   ,  -1.8955 ,  -1.617  ,  -4.07   ,\n",
      "         -0.966  ,  -3.047  ,  -3.059  ,  -2.545  ,  -2.986  ,  -2.748  ,\n",
      "         -4.99   ,  -4.113  ,  -4.926  ,  -5.438  ,  -5.16   ,  -4.137  ,\n",
      "         -4.098  ,  -3.035  ,  -3.934  ,  -3.129  ,  -3.432  ,  -1.766  ,\n",
      "         -3.887  ,  -5.08   ,  -5.258  ,  -6.195  ,  -5.14   ,  -6.496  ,\n",
      "         -3.541  ,  -3.11   ,  -3.855  ,  -3.986  ,  -2.617  ,  -1.195  ,\n",
      "         -2.244  ,  -2.91   ,  -2.459  ,  -3.676  ,   1.709  ,  -3.674  ,\n",
      "         -4.332  ,  -4.21   ,  -4.207  ,  -3.81   ,  -3.875  ,  -3.574  ,\n",
      "         -7.19   ,  -6.758  ,  -5.2    ,  -5.035  ,  -3.957  ,  -4.9    ,\n",
      "         -5.445  ,  -3.102  ,  -5.375  ,  -4.05   ,  -4.188  ,  -8.33   ,\n",
      "         -2.3    ,  -7.273  ,  -0.8813 ,  -1.469  ,  -2.809  ,  -2.22   ,\n",
      "         -1.827  ,  -1.66   ,  -0.707  ,  -1.052  ,  -0.8945 ,  -0.9014 ,\n",
      "         -2.385  ,   2.271  ,  -1.833  ,  -2.348  ,  -3.152  ,  -3.127  ,\n",
      "         -2.93   ,  -3.352  ,  -2.03   ,  -2.068  ,  -3.088  ,  -4.49   ,\n",
      "         -2.748  ,  -1.68   ,  -1.612  ,  -2.775  ,  -0.11035,   8.33   ,\n",
      "          1.181  ,  -3.24   ,  -1.676  ,  -2.912  ,  -3.03   ,  -2.     ,\n",
      "         -2.27   ,  -2.055  ,  -2.562  ,  -2.531  ,  -2.684  ,  -2.463  ,\n",
      "         -2.732  ,  -3.262  ,  -4.234  ,  -3.928  ,  -4.79   ,   1.304  ,\n",
      "         -5.566  ,  -6.062  ,  -3.912  ,  -7.7    ,  -5.332  ,  -8.08   ,\n",
      "         -6.77   ,  -5.266  ,  -5.277  ,  -7.145  ,  -8.62   ,  -5.098  ,\n",
      "         -4.62   ,  -3.281  ,  -4.594  ,  -9.03   ,  -5.156  ,  -8.56   ,\n",
      "         -6.863  ,  -6.414  ,  -7.4    ,  -6.113  ,  -7.992  ,  -5.492  ,\n",
      "         -0.478  ,  -3.36   ,  -4.17   ,  -2.842  ,  -2.434  ,  -1.724  ,\n",
      "         -3.979  ,  -3.082  ,  -0.05096,  -3.42   ,  -4.992  ,  -4.086  ,\n",
      "         -5.39   ,  -5.098  ,  -5.473  ,  -6.086  ,  -5.684  ,  -3.057  ,\n",
      "         -3.21   ,  -4.246  ,  -6.3    ,  -6.617  ,  -5.566  ,  -3.896  ,\n",
      "         -6.043  ,  -5.074  ,  -3.281  ,  -3.902  ,  -6.02   ,  -2.885  ,\n",
      "         -4.527  ,  -7.727  ,  -4.195  ,  -4.72   ,  -2.15   ,  -4.15   ,\n",
      "         -1.201  ,  -4.082  ,  -0.7705 ,  -3.74   ,   1.658  ,  -3.91   ,\n",
      "         -3.674  ,  -4.176  ,  -3.287  ,  -5.27   ,  -4.79   ,  -4.47   ,\n",
      "         -1.487  ,  -2.021  ,  -1.951  ,  -3.014  ,  -3.463  ,  -4.793  ,\n",
      "         -3.984  ,  -2.963  ,  -4.137  ,  -2.049  ,  -4.918  ,  -1.359  ,\n",
      "         -6.246  ,  -3.994  ,  -5.934  ,  -2.977  ,  -3.162  ,  -1.842  ,\n",
      "         -2.031  ,  -1.922  ,  -3.035  ,  -2.807  ,  -3.85   ,  -3.15   ,\n",
      "         -4.227  ,  -5.035  ,  -3.816  ,  -2.225  ,  -1.9375 ,  -1.351  ,\n",
      "         -2.053  ,  -2.32   ,  -3.531  ,  -4.453  ,  -3.822  ,  -5.55   ,\n",
      "         -0.4585 ,  -3.342  ,  -3.436  ,  -5.52   ,  -2.19   ,  -3.514  ,\n",
      "         -1.955  ,  -3.262  ,  -2.195  ,  -0.485  ,  -2.678  ,  -2.166  ,\n",
      "         -1.747  ,  -3.732  ,  -2.236  ,  -3.695  ,  -2.947  ,  -3.824  ,\n",
      "         -3.266  ,  -1.286  ,  -0.7764 ,  -0.0706 ,  -1.738  ,  -2.113  ,\n",
      "         -1.92   ,   0.3232 ,  -7.363  ,  -4.496  ,  -4.402  ,   3.438  ,\n",
      "         -5.734  ,  -2.932  ,  -4.703  ,  -3.604  ,  -0.87   ,  -3.613  ,\n",
      "          1.701  ,  -3.545  ,  -3.49   ,  -3.602  ,  -3.127  ,  -4.902  ,\n",
      "         -3.3    ,  -4.37   ,  -5.81   ,  -4.46   ,  -3.098  ,  -2.615  ,\n",
      "         -0.9277 ,  -2.047  ,  -2.582  ,  -2.434  ,  -4.996  ,  -7.195  ,\n",
      "         -5.49   ,  -6.11   ,  -6.996  ,  -5.73   ,  -2.709  ,  -2.477  ,\n",
      "         -4.562  ,  -2.145  ,  -2.549  ,  -2.242  ,  -3.13   ,  -3.537  ,\n",
      "         -0.3333 ,  -4.105  ,  -3.791  ,  -4.246  ,  -5.56   ,  -4.633  ,\n",
      "         -2.434  ,  -5.363  ,  -6.633  ,  -4.016  ,  -3.05   ,  -4.58   ,\n",
      "         -4.09   ,  -1.205  ,  -2.41   ,  -1.843  ,  -1.893  ,  -4.926  ,\n",
      "         -7.773  ,   0.2222 ,  -9.38   ,  -4.402  ,  -5.89   ,   2.828  ,\n",
      "         -2.55   ,  -5.06   ,  -7.008  ,  -2.582  ,  -5.5    ,  -3.543  ,\n",
      "         -1.582  ,  -2.512  ,  -5.133  ,  -2.766  ,  -3.68   ,  -3.281  ,\n",
      "         -6.754  ,  -3.756  ,  -3.85   ,  -3.213  ,  -4.88   ,  -7.188  ,\n",
      "         -4.734  ,  -3.988  ,  -3.018  ,  -3.137  ,  -3.666  ,  -2.426  ,\n",
      "         -3.59   ,  -3.43   ,  -3.148  ,  -6.324  ,  -7.34   ,  -8.266  ,\n",
      "         -2.998  ,  -6.406  ,   3.66   ,  -2.162  ,  -7.43   ,  -2.91   ,\n",
      "         -3.234  ,  -3.021  ,  -5.99   ,  -2.713  ,  -1.56   ,  -2.383  ,\n",
      "         -3.734  ,   3.234  ,  -1.221  ,  -5.992  ,  -4.56   ,  -2.457  ,\n",
      "         -4.938  ,  -2.438  ,  -4.79   ,  -6.28   ,  -5.484  ,  -4.254  ,\n",
      "         -6.44   ,  -2.018  ,  -1.886  ,  -2.574  ,  -5.547  ,  -3.91   ,\n",
      "         -2.916  ,  -2.564  ,  -5.21   , -10.     ,  -5.145  ,  -9.555  ,\n",
      "         -6.047  ,  -5.9    ,  -5.688  ,  -3.979  ,  -4.6    ,  -6.832  ,\n",
      "         -4.71   ,  -2.533  ,  -6.445  ,   2.41   ,  -2.076  ,  -0.4749 ,\n",
      "         -3.215  ,  -2.291  ,  -1.967  ,  -3.295  ,  -2.215  ,  -2.674  ,\n",
      "         -3.979  ,  -3.488  ,  -3.96   ,  -3.965  ,  -2.629  ,  -2.771  ,\n",
      "         -0.4883 ,  -2.223  ,  -3.83   ,  -2.502  ,  -4.03   ,  -3.207  ,\n",
      "         -3.523  ,  -1.825  ,  -7.387  ,  -2.135  ,  -3.824  ,  -1.197  ,\n",
      "         -0.701  ,  -9.28   ,   0.178  ,  -1.171  ,  -0.763  ,  -5.69   ,\n",
      "          0.9414 ,  -2.094  ,  -0.3752 ,  -1.773  ,  -1.485  ,  -4.03   ,\n",
      "         -4.184  ,  -3.5    ,  -2.787  ,  -3.102  ,  -4.3    ,  -2.328  ,\n",
      "          0.7817 ,  -0.0841 ,  -0.302  ,  -1.238  ,  -1.114  ,  -2.78   ,\n",
      "         -3.52   ,  -3.098  ,  -9.41   ,  -1.295  ,  -4.746  ,  -4.043  ,\n",
      "         -2.951  ,  -3.205  ,  -2.521  ,  -5.285  ,  -0.9126 ,   0.4731 ,\n",
      "         -2.758  ,   2.361  ,  -3.518  ,  -4.082  ,  -4.883  ,  -3.627  ,\n",
      "         -3.463  ,  -4.64   ,  -3.646  ,  -0.5615 ,  -1.314  ,   0.5537 ,\n",
      "         -1.603  ,  -0.825  ,  -2.54   ,  -3.895  ,  -6.844  ,  -3.549  ,\n",
      "         -5.742  ,  -2.195  ,  -3.242  ,  -6.2    ,  -1.384  ,  -4.87   ,\n",
      "         -1.233  ,   0.2622 ,  -1.657  ,   0.525  ,  -1.509  ,  -2.166  ,\n",
      "         -1.749  ,  -1.566  ,  -2.428  ,  -2.75   ,  -4.55   ,  -0.939  ,\n",
      "         -6.613  ,  -5.605  ,  -3.697  ,  -4.03   ,  -2.85   ,  -2.8    ,\n",
      "         -2.873  ,  -5.773  ,   5.49   ,  -0.4275 ,  -5.95   ,  -2.818  ,\n",
      "         -3.346  ,  -0.3828 ,   0.5615 ,  -0.764  ,  -0.796  ,  -0.8647 ,\n",
      "         -0.461  ,  -2.166  ,  -0.3782 ,  -1.897  ,  -1.772  ,  -3.324  ,\n",
      "         -1.718  ,  -2.445  ,  -2.393  ,  -2.17   ,  -2.633  ,  -2.75   ,\n",
      "         -1.547  ,  -1.542  ,  -1.195  ,  -0.569  ,  -2.883  ,   9.555  ,\n",
      "          1.18   ,  -2.6    ,  -4.152  ,  -5.344  ,  -2.95   ,   0.1164 ,\n",
      "         -1.172  ,  -0.9717 ,  -2.617  ,  -1.237  ,  -2.66   ,  -1.534  ,\n",
      "         -3.043  ,  -2.645  ,  -3.71   ,  -4.414  ,  -1.276  ,  -2.875  ,\n",
      "         -4.113  ,  -3.531  ,  -3.713  ,  -2.916  ,  -6.42   ,  -6.895  ,\n",
      "         -7.15   ,  -3.777  ,  -4.75   ,  -5.73   ,  -4.316  ,  -5.473  ,\n",
      "          5.86   ,  -1.324  ,  -4.63   ,  -3.096  ,  -3.31   ,  -5.695  ,\n",
      "         -4.617  ,  -2.582  ,  -2.553  ,  -0.177  ,   0.1929 ,   8.43   ,\n",
      "          7.668  ,   0.5015 ,   7.652  ,   6.855  ,  -3.523  ,  -0.6846 ,\n",
      "         -3.205  ,   6.82   ,  -3.266  ,   6.004  ,  -0.77   ,   8.086  ,\n",
      "         -4.766  ,   0.3398 ,  -1.393  ,  -1.514  ,   1.251  ,  -2.1    ,\n",
      "         -2.273  ,  -1.1045 ,   2.742  ,   8.91   ,   1.926  ,   7.86   ,\n",
      "         -5.71   ,   5.805  ,  -4.465  ,  -0.0676 ,  -6.516  ,   6.164  ,\n",
      "         -0.5723 ,   1.675  ,   4.637  ,   1.542  ,  -0.676  ,   0.4575 ,\n",
      "          0.798  ,  -6.48   ,  -0.802  ,  -5.043  ,  -1.65   ,  -4.727  ,\n",
      "         -1.9795 ,  -1.727  ,  -0.979  ,  -6.72   ,  -4.098  ,  -4.16   ,\n",
      "         -6.29   ,  -7.22   , -10.88   ,  -8.555  ,  -5.188  ,  -3.496  ,\n",
      "         -0.3503 ,  -1.342  ,   0.7705 ,  -4.723  ,   1.125  ,  -3.074  ,\n",
      "         -7.42   ,  -3.52   ,   0.838  ,   9.14   ,   7.26   ,   8.34   ,\n",
      "          5.406  ,   4.89   ,   5.066  ,   5.4    ,  -2.428  ,   5.742  ,\n",
      "         -3.406  ,   6.156  ,  -4.086  ,   8.77   ,   6.312  ,   4.934  ,\n",
      "         -4.3    ,   2.846  ,   4.25   ,   3.137  ,   4.2    ,   8.32   ,\n",
      "          6.883  ,   4.293  ,  -6.99   ,   9.555  ,   7.688  ,   8.12   ,\n",
      "         -6.22   ,   7.305  ,  -7.344  ,   7.312  ,  -6.566  ,   6.72   ,\n",
      "          1.734  ,   0.1459 ,   1.138  ,  -1.34   ,  -1.989  ,  -2.145  ,\n",
      "         -1.75   ,  -2.03   ,  -4.12   ,  -2.576  ,  -1.337  ,  -3.893  ,\n",
      "         -4.004  ,  -4.457  ,  -2.51   ,  -1.907  ,  -5.453  ,  -4.562  ,\n",
      "         -5.91   ,  -8.266  ,  -6.152  ,  -1.071  ,  -3.605  ,  -0.849  ,\n",
      "         -4.586  ,  -1.466  ,  -1.273  ,  -3.035  ,  -2.223  ,   2.463  ,\n",
      "         -0.3923 ,   0.7646 ,  -1.462  ,  -0.2091 ,  -2.186  ,  -1.984  ,\n",
      "         -3.117  ,  -1.986  ,  -1.997  ,  -3.074  ,  -2.373  ,  -1.366  ,\n",
      "         -1.094  ,  -0.9604 ,  -1.351  ,  -1.883  ,  -4.465  ,  -2.605  ,\n",
      "         -3.137  ,   0.2003 ,   7.75   ,  -0.2822 ,  -2.963  ,   2.771  ,\n",
      "          0.3442 ,   0.538  ,  -0.3987 ,   0.04724,   0.2952 ,  -1.821  ,\n",
      "         -1.006  ,  -1.912  ,  -1.128  ,  -2.32   ,  -3.467  ,  -2.328  ,\n",
      "         -4.742  ,  -2.885  ,   5.695  ,  -1.25   ,  -0.9453 ,  -1.356  ,\n",
      "          0.1937 ,  -0.3616 ,  -2.021  ,   7.082  ,  -5.145  ,  -3.244  ,\n",
      "         -3.55   ,  -1.536  ,  -3.613  ,  -1.783  ,  -5.23   ,  -1.05   ,\n",
      "         -4.746  ,  -1.505  ,  -3.885  ,  -7.418  ,  -3.025  ,  -4.32   ,\n",
      "         -6.62   ,  -8.68   ,  -7.4    ,  -6.133  ,  -8.22   ,  -3.684  ,\n",
      "         -3.158  ,  -2.465  ,  -1.404  ,  -2.162  ,  -1.732  ,  -7.008  ,\n",
      "        -10.45   ,  -6.945  ,  -4.906  ,  -0.1708 ,  -1.421  ,   0.674  ,\n",
      "         -1.806  ,   0.769  ,  -2.393  ,  -2.953  ,  -6.418  ,  -4.09   ,\n",
      "         -2.793  ,  -4.402  ,  -3.742  ,  -3.69   ,  -6.9    ,  -5.832  ,\n",
      "         -7.32   ,  -3.404  ,  -4.72   ,   3.686  ,  -4.77   ,  -1.788  ,\n",
      "         -0.991  ,  -1.714  ,  -1.384  ,  -3.783  ,  -8.664  ,  -1.6875 ,\n",
      "         -2.627  ,  -4.51   ,  -2.11   ,   0.0732 ,  -3.766  ,  -1.557  ,\n",
      "         -1.761  ,  -3.611  ,  -3.158  ,  -2.656  ,  -2.639  ,  -6.645  ,\n",
      "         -3.191  ,  -3.354  ,  -5.766  ,  -4.684  ,  -8.09   ,  -3.092  ,\n",
      "         -6.184  ,  -1.523  ,   3.2    ,  -2.57   ,  -2.812  ,  -3.492  ,\n",
      "         -2.281  ,  -1.977  ,  -6.527  ,  -4.703  ,  -2.861  ,  -8.36   ,\n",
      "         -4.035  ,   1.086  ,   6.367  ,  -0.5283 ,  -0.4211 ,  -2.043  ,\n",
      "          0.3467 ,  -2.658  ,  -4.285  ,  -4.23   ,  -1.42   ,  -3.4    ,\n",
      "         -6.555  ,  -3.56   ,  -6.168  ,  -3.064  ,  -9.766  ,  -4.234  ,\n",
      "          0.8896 ,   2.96   ,  -1.826  ,  -4.25   ,  -2.934  ,  -2.146  ,\n",
      "          0.1351 ,  -2.865  ,  -1.894  ,  -7.496  ,  -3.316  ,  -2.43   ,\n",
      "         -0.7417 ,   1.993  ,   0.913  ,   1.0625 ,  -3.79   ,  -1.676  ,\n",
      "         -4.336  ,  -2.648  ,  -2.096  ,  -6.895  ,  -2.697  ,  -2.525  ,\n",
      "         -4.01   ,  -1.142  ,  -9.93   ,  -2.957  ,  -3.926  ,  -2.08   ,\n",
      "          2.69   ,  -2.578  ,  -3.05   ,  -2.314  ,  -3.242  ,  -2.303  ,\n",
      "         -3.36   ,  -8.17   ,  -0.4028 ,  -1.931  ,  -3.047  ,   1.434  ,\n",
      "          0.2893 ,   0.712  ,  -0.648  ,  -0.8687 ,  -1.427  ,  -4.375  ,\n",
      "         -2.082  ,  -4.48   ,  -2.701  ,  -3.14   ,  -4.14   ,  -3.025  ,\n",
      "        -10.57   ,  -3.295  ,  -7.246  ,  -0.861  ,   2.19   ,  -0.408  ,\n",
      "          0.1732 ,   0.3096 ,  -1.838  ,  -2.969  ,  -4.418  ,  -3.463  ,\n",
      "         -0.2817 ,  -4.52   ,   0.6636 ,  -2.104  ,   0.4043 ,  -2.473  ,\n",
      "         -1.698  ,  -4.504  ,  -1.125  ,  -5.     ,  -2.707  ,  -3.316  ,\n",
      "         -5.38   ,  -4.867  ,  -4.566  ,  -6.438  ,  -1.613  ,  -1.346  ,\n",
      "         -1.242  ,  -0.4858 ,  -1.092  ,  -2.195  ,  -3.621  ,  -3.287  ,\n",
      "         -4.164  ,  -1.406  ,  -1.109  ,   0.511  ,  -1.429  ,   0.534  ,\n",
      "         -1.551  ,  -0.492  ,  -2.273  ,  -1.159  ,  -3.018  ,  -1.121  ,\n",
      "         -4.156  ,  -0.3699 ,  -2.113  ,  -3.596  ,   5.8    ,   1.991  ,\n",
      "         -9.27   ,  -5.203  ,  -3.213  ,  -4.465  ,  -1.377  ,  -1.706  ,\n",
      "         -0.859  ,   1.869  ,   0.4558 ,   2.701  ,   0.3586 ,   2.475  ,\n",
      "         -0.0913 ,   1.923  ,  -0.0463 ,   1.48   ,  -0.653  ,  -0.2915 ,\n",
      "         -0.5786 ,   1.452  ,   1.789  ,  13.914  ,   9.125  ,  -0.5557 ,\n",
      "          6.43   ,   4.477  ,   1.0625 ,   2.992  ,   0.673  ,   2.492  ,\n",
      "          1.832  ,  -0.02058,  -0.7007 ,   1.276  ,  -0.09326,  -1.062  ,\n",
      "         -0.186  ,  -0.2668 ,  -1.352  ,  -2.424  ,  -2.285  ,  -2.104  ,\n",
      "         -2.309  ,  -0.4297 ,  -3.312  ,   6.387  ,   1.18   ,  -2.111  ,\n",
      "          0.2974 ,  -5.543  ,  -4.883  ,  -0.9883 ,  -2.957  ,  -2.658  ,\n",
      "         -2.38   ,  -0.8506 ,   1.338  ,   0.9253 ,   0.288  ,  -0.454  ,\n",
      "         -0.1443 ,  -1.832  ,  -1.387  ,  -2.121  ,  -1.2295 ,  -1.735  ,\n",
      "         -2.205  ,  -0.8906 ,  -2.44   ,   5.75   ,   2.027  ,  -1.673  ,\n",
      "         -0.435  ,  -0.3513 ,  -2.549  ,  -1.912  ,   0.03485,  -7.734  ,\n",
      "         -5.44   ,  -3.91   ,  -3.94   ,  14.016  ,  14.234  ,   9.47   ,\n",
      "          3.86   ,   9.1    ,   1.8    ,   6.582  ,   2.488  ,   3.006  ,\n",
      "         -1.707  ,   7.105  ,  -1.728  ,   2.9    ,   1.604  ,   2.89   ,\n",
      "          8.13   ,   3.248  ,   0.621  ,   4.81   ,   6.48   ,   9.14   ,\n",
      "          6.61   ,   7.125  ,   7.723  ,   7.62   ,   1.127  ,   0.7173 ,\n",
      "          0.4895 ,  -1.208  ,  -0.10364,  -2.791  ,  -2.703  ,  -2.72   ,\n",
      "         -1.763  ,  -1.687  ,  -3.688  ,  -2.723  ,  -3.047  ,  -3.32   ,\n",
      "          0.722  ,  -2.215  ,  -2.25   ,  -1.492  ,  -5.91   ,  -6.383  ,\n",
      "         -4.062  ,  -7.703  ,  -8.805  ,  -2.387  ,  -1.509  ,  -0.3105 ,\n",
      "          0.9023 ,  -2.25   ,   0.2537 ,  -2.06   ,  -2.023  ,  -2.848  ,\n",
      "          0.7705 ,  -3.838  ,  -1.125  ,  -4.344  ,  -3.092  ,  -4.367  ,\n",
      "         -0.913  ,  -2.086  ,  -2.707  ,  -3.68   ,  -0.771  ,  -0.676  ,\n",
      "         -1.948  ,  -2.293  ,  -4.617  ,  -3.727  ,  -3.172  ,   0.7876 ,\n",
      "         -0.565  ,   0.721  ,  -2.426  ,   0.5986 ,  -2.814  ,  -0.7964 ,\n",
      "         -3.516  ,  -0.183  ,  -3.035  ,  -3.559  ,  -2.875  ,  -3.35   ,\n",
      "         -2.346  ,  -1.84   ,  -2.107  ,  -1.223  ,  -1.408  ,  -0.8604 ,\n",
      "         -0.9805 ,  -0.98   ,  -1.098  ,  -2.494  ,   8.21   ,   3.527  ,\n",
      "          2.475  ,   2.5    ,  -0.657  ,  -1.632  ,  -1.988  ,  -1.75   ,\n",
      "         -2.254  ,  -2.188  ,  -0.9062 ,  -0.4937 ,  -1.712  ,  -1.693  ,\n",
      "         -2.219  ,  -0.981  ,  -1.639  ,  -2.139  ,   1.362  ,  -0.532  ,\n",
      "         -0.5073 ,  -1.114  ,  -1.468  ,  -2.021  ,   0.879  ,  -0.9736 ,\n",
      "         -1.865  ,   3.021  ,   1.703  ,   0.9214 ,   0.02968,  -1.12   ,\n",
      "         -1.8545 ,  -0.4473 ,  -0.9688 ,  -1.787  ,  -0.832  ,   0.3245 ,\n",
      "          1.249  ,  -0.8867 ,  -0.999  ,  -1.805  ,  -0.4531 ,  -1.288  ,\n",
      "         -1.918  ,   2.492  ,   1.209  ,   0.767  ,   0.1926 ,  -1.077  ,\n",
      "         -1.913  ,  -0.7563 ,  -1.428  ,  -2.078  ,   1.99   ,   0.3237 ,\n",
      "         -0.02675,  -0.9976 ,  -1.5    ,  -2.137  ,   0.01921,  -1.145  ,\n",
      "         -1.676  ,   1.162  ,   0.547  ,   0.1339 ]], dtype=float16), array([[ 0.1262, -1.207 ,  1.078 ]], dtype=float16)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_data = np.array(data)\n",
    "print(model_data.shape)\n",
    "model_data = np.expand_dims(model_data, axis=0)\n",
    "print(model_data.shape)\n",
    "\n",
    "\n",
    "prediction = loaded_model.predict([model_data])\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
